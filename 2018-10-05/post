
-- REDES NEURAIS INTERPRETÃVEIS --

Na Ãºltima aula com Luiz Eduardo Soares de Oliveira aprendemos sobre o Perceptron

O Perceptron Ã© um algoritmo de aprendizado supervisionado de classificaÃ§Ã£o binÃ¡ria (se um input pertence ou nÃ£o a uma classe). Ã‰ a primeira e mais primitiva estrutura de rede neural, sendo desenvolvido em 1957.


Supondo uma funÃ§Ã£o de ativaÃ§Ã£o do tipo hardlim com f(x)=1 se x>=0, e f(x)=0 se x<0 e pesos iniciais sendo W0=(0,0) com viÃ©s b0=0, podemos calcular os seguintes passos (dados na imagem):


1a linha (x1):
y=hardlim([0,0] * [2,2] + 0 = hardlim(0) = 1
Como a label era 0, o erro Ã© a label menos a previsÃ£o. Erro=0-1=-1
Com erro != 0, atualiza-se os pesos W1 = W0 + e*x1 = [0,0] + (-1 * [2,2]) = [-2,-2]
Atualiza-se o bias b1 = b0 + Erro = 0 -1 = -1

2a linha (x2):
y=hardlim([-2,-2] * [-2,-2] -1 = hardlim(7) = 1
Erro Ã© zero, peso e bias nÃ£o atualizam

3a linha (x3):
y=hardlim([-2,-2] * [-2,2] -1 = hardlim(-1) = 0
Erro Ã© zero, peso e bias nÃ£o atualizam

4a linha (x4)
y=hardlim([-2,-2] * [-1,1] -1 = hardlim(-1) = 0
Erro = label - previsÃ£o = 1 - 0 = 1
W2 = W1 + e*x4 = [-2,-2] + 1*[-1,1] = [-3,3]
b2 = b1 + e = -1 + 1 = 0

Depois de 5 iteraÃ§Ãµes, o modelo convergirÃ¡ aos melhores parÃ¢metros W*=[-1,-3] e b*=3! ğŸ¤–ğŸ‰ğŸ‰
